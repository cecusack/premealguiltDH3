---
title: "mealtime emotions"
output: html_document
date: "2023-10-03"
---

# libs dat
```{r}
devtools::install_github("rpsychologist/powerlmm")
library(powerlmm)
if (!require("tidyverse")) {install.packages("tidyverse"); require("tidyverse")} # tidy
if (!require("psych")) {install.packages("psych"); require("psych")} # descriptives
if (!require("lme4")) {install.packages("lme4"); require("lme4")} # fits mixed models
if (!require("nlme")) {install.packages("nlme"); require("nlme")} # for fitting multiple groups / heteroscedastic model
if (!require("lmerTest")) {install.packages("lmerTest"); require("lmerTest")} # provides t-tests for fixed effects
if (!require("performance")) {install.packages("performance"); require("performance")} # computes ICC
if (!require("DataCombine")) {install.packages("DataCombine"); require("DataCombine")} # leadingDataCombine
# library(plyr)
# library(sjPlot)
library(multcomp)
library(psychometric)
# library(EMAtools)
library(r2mlm)

dat <- read.csv("mlmdat_statetraitcent.csv", header = TRUE, sep = ",", na.strings=c("", "NA", "N/A", "n/a", "N/a", "-99"), stringsAsFactors = TRUE)

# load("premealemotions.RData")
```

# functions
```{r}
#### effect size ####
lme.dscore<-function(mod,data,type){
  if (type=="lme4") {
    mod1<-lmerTest::lmer(mod,data=data)
    eff<-cbind(summary(mod1)$coefficients[,4],summary(mod1)$coefficients[,3])
  }

  if (type=="nlme") {
    eff=cbind(summary(mod)$tTable[,4],summary(mod)$fixDF$terms)
  }

  colnames(eff)<-c("t","df")
  eff<-as.data.frame(eff)
  eff$d<-(2*eff$t)/sqrt(eff$df)
  eff<-eff[-1,]
  return(eff)
}

samplesize_mixed <- function(eff.size, df.n = NULL, power = .8, sig.level = .05, k, n, icc = 0.05) {
  if (!requireNamespace("pwr", quietly = TRUE)) {
    stop("Package `pwr` needed for this function to work. Please install it.", call. = FALSE)
  }

  # compute sample size for standard design
  if (is.null(df.n))
    # if we have no degrees of freedom specified, use t-test
    obs <- 2 * pwr::pwr.t.test(d = eff.size, sig.level = sig.level, power = power)$n
  else
    # we have df, so power-calc for linear models
    obs <- pwr::pwr.f2.test(u = df.n, f2 = eff.size, sig.level = sig.level, power = power)$v + df.n + 1

  # if we have no information on the number of observations per cluster,
  # compute this number now
  if (missing(n) || is.null(n)) {
    n <- (obs * (1 - icc)) / (k - (obs * icc))
    if (n < 1) {
      warning("Minimum required number of subjects per cluster is negative and was adjusted to be positive. You may reduce the requirements for the multi-level structure (i.e. reduce `k` or `icc`), or you can increase the effect-size.", call. = FALSE)
      n <- 1
    }
  }

  # adjust standard design by design effect
  total.n <- obs * design_effect(n = n, icc = icc)


  # sample size for each group and total n
  smpsz <- list(round(total.n / k), round(total.n))

  # name list
  names(smpsz) <- c("Subjects per Cluster", "Total Sample Size")
  smpsz
}


#' @rdname samplesize_mixed
#' @export
smpsize_lmm <- samplesize_mixed

design_effect <- function(n, icc = 0.05) {
  1 + (n - 1) * icc
}

lme.dscore<-function(mod,data,type){
  if (type=="lme4") {
    mod1<-lmerTest::lmer(mod,data=data)
    eff<-cbind(summary(mod1)$coefficients[,4],summary(mod1)$coefficients[,3])
  }

  if (type=="nlme") {
    eff=cbind(summary(mod)$tTable[,4],summary(mod)$fixDF$terms)
  }

  colnames(eff)<-c("t","df")
  eff<-as.data.frame(eff)
  eff$d<-(2*eff$t)/sqrt(eff$df)
  eff<-eff[-1,]
  return(eff)
}

#### power curve ####
ema.powercurve=function(NumbPart,NumbResp,days,respday,Est_ICC=.05,COL.8="red",COL.5="blue",COL.2="green"){


  if(!missing(days) & !missing(respday)) {
    NumbResp<-days*respday
  } else {
    NumbResp<-NumbResp
  }

  ### initate matricies ####
  eff8a<-NULL;eff2a<-NULL;eff5a<-NULL

  #### functions for power curves ####

  for (PWR in c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.99)){
    eff8<-(smpsize_lmm(eff.size = 0.8, power = PWR, sig.level = 0.05, k = NumbPart, icc = Est_ICC,n=NumbResp))
    eff8a<-as.data.frame(rbind(eff8a,eff8$`Subjects per Cluster`))
  }

  for (PWR in c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.99)){
    eff5<-(smpsize_lmm(eff.size = 0.5, power = PWR, sig.level = 0.05, k = NumbPart, icc = Est_ICC,n=NumbResp))
    eff5a<-as.data.frame(rbind(eff5a,eff5$`Subjects per Cluster`))
  }


  for (PWR in c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.99)){
    eff2<-(smpsize_lmm(eff.size = 0.2, power = PWR, sig.level = 0.05, k = NumbPart, icc = Est_ICC,n=NumbResp))
    eff2a<-as.data.frame(rbind(eff2a,eff2$`Subjects per Cluster`))
  }


  for (Add99 in c(10,20,30,40,50,60,70,75,80,85,90,95,100,105,110,115,120,125,130,140,150,160,170)){
    eff2a<-as.data.frame(rbind(eff2a,(eff2$`Subjects per Cluster`+Add99)))
    eff5a<-as.data.frame(rbind(eff5a,(eff5$`Subjects per Cluster`+Add99)))
    eff8a<-as.data.frame(rbind(eff8a,(eff8$`Subjects per Cluster`+Add99)))
  }

  ### merging curves ###
  power<-rbind(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99)

  ### creating response rate lines


  NumbRespColumn<-cbind(rep(c((NumbResp*.50),(NumbResp*.75),(NumbResp)),each=34))

  LabResp<-cbind(rep(c("50%","75%","100%"),each=34))

  comp_final<-as.data.frame(cbind((rbind(power,power,power)),NumbRespColumn,LabResp))
  colnames(comp_final)<-c("power","NumbRespColumn","Response_Rate")


  lg<-data.frame(cbind(power,eff8a,"Large (d=0.8)"));colnames(lg)<-c("Power","Resp","Effect_Size")
  md<-data.frame(cbind(power,eff5a,"Medium (d=0.5)"));colnames(md)<-c("Power","Resp","Effect_Size")
  sm<-data.frame(cbind(power,eff2a,"Small (d=0.2)"));colnames(sm)<-c("Power","Resp","Effect_Size")

  eff_final<-rbind(lg,md,sm)



  #### create ggplot ###

  xlab_chart <- paste("Responses per participant (n =",NumbPart,"participants)" )

  #  ggplot2::scale_x_continuous(limits = c(0,(round((NumbResp+40),-1))),breaks =seq(0, (round(NumbResp+40,-1)), by=20))+
  #round_any(NumbResp, 10, f = ceiling)

  if(NumbResp<=10) {Figure_X_Limit<-10}
  if (NumbResp>10 & NumbResp<=15) {Figure_X_Limit<-15}
  if (NumbResp>15 & NumbResp<=20) {Figure_X_Limit<-20}
  if(NumbResp>20) {Figure_X_Limit<-(plyr::round_any(NumbResp, 10, f = ceiling)+10)}

  if(max(eff_final[(eff_final$Resp<NumbResp & eff_final$Effect_Size=="Large (d=0.8)"),]$Power)==0.99) {
    eff_final<-rbind(eff_final,data.frame(Power=0.99,Resp=NumbResp,Effect_Size="Large (d=0.8)"))}

  if(max(eff_final[(eff_final$Resp<NumbResp & eff_final$Effect_Size=="Medium (d=0.5)"),]$Power)==0.99) {
    eff_final<-rbind(eff_final,data.frame(Power=0.99,Resp=NumbResp,Effect_Size="Medium (d=0.5)"))}



  PowerPlot1<-ggplot2::ggplot()+ ggplot2::geom_line(ggplot2::aes(x = Resp,y = Power,color=Effect_Size),size=1, data=eff_final[eff_final$Resp<=NumbResp,])+
    ggplot2::xlab(xlab_chart) +
    ggplot2::ylab("Power (1-beta)") +
    ggplot2::scale_y_continuous(breaks=c(0,0.2,0.4,0.6,0.8,1.00), limits=c(0.1,1.00))+
    ggplot2::geom_vline(xintercept=(NumbResp*.50),color="grey65", linetype = 3)+
    ggplot2::geom_vline(xintercept=(NumbResp*.75),color="grey65", linetype = 2)+
    ggplot2::scale_x_continuous(limits = c(0,Figure_X_Limit))+
    ggplot2::geom_vline(xintercept=(NumbResp),color="grey65", linetype = 1)+
    ggplot2::geom_line(ggplot2::aes(x = as.numeric(NumbRespColumn), y = as.numeric(power), linetype=Response_Rate), data=comp_final,color="grey65")+
    ggplot2::theme_classic() + ggplot2::scale_linetype(name="Completion rate") +
    ggplot2::scale_color_manual(name="Effect Size",values=c(COL.8,COL.5,COL.2))
  return(PowerPlot1)
}

within_between_alpha <- function(data,participant){
  # Load libraries
  require(psych)
  #require(alpha)
  require(dplyr)
  
  # Get between-person alpha
  between_person_alpha = psych::alpha(as.data.frame(lapply(select(data,-participant),as.numeric)),check.keys=TRUE)$total
  
  # Get within-person alpha
  within_person_alphas = apply(unique(data[participant]),1,function(x){
    select(subset(data,data[participant]==x),-participant)
    psych::alpha(as.data.frame(lapply(select(subset(data,data[participant]==x),-participant),as.numeric)),check.keys=TRUE)$total
  })
  
  # Create results
  results = data.frame(matrix(nrow=3,ncol=length(between_person_alpha)+1))
  colnames(results) = c('type',colnames(between_person_alpha))
  
  # Store results
  results$type = c('between_person','within_person_mean','within_person_median')
  results[1,2:(length(between_person_alpha)+1)] = between_person_alpha
  results[2,2:(length(between_person_alpha)+1)] = apply(bind_rows(within_person_alphas),2,function(x) mean(x,na.rm=T))
  results[3,2:(length(between_person_alpha)+1)] = apply(bind_rows(within_person_alphas),2,function(x) median(x,na.rm=T))
  
  # Return results
  return(results)
}

#### EXAMPLE ####

# Negative affect
# negative_affect_data <- my_data_ml[,c('Participant','affect_angstig','affect_eenzaam','affect_gestrest','affect_schuldig','affect_somber','affect_onzeker')]
# alpha_na <- within_between_alpha(negative_affect_data,'Participant')
```

# Model Construction
## Anxiety-Avoidance cycle
### H1a: anx to avoid
```{r}
#### ICC ####
ICC1.lme(avoid_lead, unique_id, data=dat) # 0.65

#### intercept only model ####
avoid1 = lme(avoid_lead ~ 1,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(avoid1)

#### model 2: add level 1 effects of state variables and emotional avoidance at the previous time point ####
avoid2 = lme(avoid_lead ~ 1 + avoid_state + anx_state,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(avoid2)
intervals(avoid2)
anova(avoid1, avoid2)

#### model 3: add level 2 effects ####
avoid3 = lme(avoid_lead ~ 1 + avoid_state + anx_state + anx_trait,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(avoid3)
intervals(avoid3)
anova(avoid2, avoid3)


#### model 4: add random slopes for state var ####
# avoid4 = lme(avoid_lead ~ 1 + avoid_state + anx_state + cent_anx,
#             random =~ 1 + anx_state|unique_id,
#             data=dat,
#             correlation= corCAR1(form=~beepcont),
#             na.action = na.exclude)
# summary(avoid4)
# intervals(avoid4) # NPD
# anova(avoid3, avoid4)

qqnorm(residuals(avoid3))
qqnorm(avoid3, ~ranef(., level=1))
plot(avoid3)
```

### H1b: avoid to anx
```{r}
#### ICC ####
ICC1.lme(anx_lead, unique_id, data=dat) # 0.54

#### intercept only model ####
anx1 = lme(anx_lead ~ 1,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(anx1)

#### model 2: add level 1 effects of state variables and emotional avoidance at the previous time point ####
anx2 = lme(anx_lead ~ 1 + anx_state + avoid_state,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(anx2)
anova(anx1, anx2)

#### model 3: add level 2 effects ####
anx3 = lme(anx_lead ~ 1 + anx_state + avoid_state + cent_avoid,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(anx3)
intervals(anx3)
exp(attr(avoid3$apVar, "Pars"))^2
anova.lme(anx2, anx3)


#### model 4: add random slopes for state var ####
# anx4 = lme(anx_lead ~ 1 + anx_state + avoid_state + cent_avoid,
#             random =~ 1 + avoid_state|unique_id,
#             data=dat,
#             na.action = na.exclude)
# summary(anx4)
# intervals(anx4) # NPD too complex go back to anx 3

qqnorm(residuals(anx3))
qqnorm(anx3, ~ranef(., level=1))
plot(anx3)
```

## Anx guilt avoid
### H2a: anx guilt to avoid
```{r}
avoidcomb = lme(avoid_lead ~ 1 + avoid_state + anx_state +guilt_state,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(avoidcomb)

#### model 2: add level 1 effects ####
avoidcomb2 = lme(avoid_lead ~ 1 + avoid_state + anx_state + cent_anx + guilt_state+cent_guilt,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(avoidcomb2)
intervals(avoidcomb2)

#### model 3: add random slopes for state var ####
# avoidcomb3 = lme(avoid_lead ~ 1 + avoid_state + anx_state + cent_anx+ guilt_state+cent_guilt,
#             random =~ 1 + anx_state+guilt_state|unique_id,
#             data=dat,
#             correlation= corCAR1(form=~beepcont),
#             na.action = na.exclude)
# summary(avoidcomb3)
# intervals(avoidcomb3) # NPD

qqnorm(residuals(avoidcomb2))
qqnorm(avoidcomb2, ~ranef(., level=1))
plot(avoidcomb2)
```

### H2b: avoid to guilt
```{r}
guilt1 = lme(guilt_lead ~ 1,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(guilt1)

#### model 2: add level 1 effects of state variables and guilt at the previous time point  ####
guilt2 = lme(guilt_lead ~ 1 + guilt_state + avoid_state,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(guilt2)

#### model 3: add level 2 effects ####
guilt3 = lme(guilt_lead ~ 1 + guilt_state + avoid_state + cent_avoid,
            random =~ 1|unique_id,
            data=dat,
           # correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(guilt3)
intervals(guilt3)
anova(guilt2,guilt3)

#### model 4: add random slopes for state var ####
# guilt4 = lme(guilt_lead ~ 1 + guilt_state + avoid_state + cent_avoid,
#             random =~ 1 + avoid_state|unique_id,
#             data=dat,
#             na.action = na.exclude)
# summary(guilt4)
# intervals(guilt4)
# anova(guilt3,guilt4)

qqnorm(residuals(guilt3))
qqnorm(guilt3, ~ranef(., level=1))
plot(guilt3)
```

## Phys and avoid
### H3a: phys to avoid
```{r}
avoid_phys = lme(avoid_lead ~ 1 + avoid_state + phys_state,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(avoid_phys)
intervals(avoid_phys)

### add level 2 ###
avoid_phys2 = lme(avoid_lead ~ 1 + avoid_state + phys_state+cent_phys,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(avoid_phys2)
intervals(avoid_phys2)

avoid_phys2_noAR1 = lme(avoid_lead ~ 1 + avoid_state + phys_state+cent_phys,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(avoid_phys2_noAR1)
intervals(avoid_phys2_noAR1)

anova(avoid_phys2, avoid_phys2_noAR1)

### add random slopes ###
# avoid_phys3 = lme(avoid_lead ~ 1 + avoid_state + phys_state+cent_phys,
#             random =~ 1 + phys_state|unique_id,
#             data=dat,
#             correlation= corCAR1(form=~beepcont),
#             na.action = na.exclude)
# summary(avoid_phys3)
# intervals(avoid_phys3)
# anova(avoid_phys2, avoid_phys3)

qqnorm(residuals(avoid_phys2_noAR1))
qqnorm(avoid_phys2_noAR1, ~ranef(., level=1))
plot(avoid_phys2_noAR1)
```

### H3b: avoid to phys
```{r}
#### ICC ####
ICC1.lme(phys_lead, unique_id, data=dat) # 0.65

#### intercept only model ####
phys1 = lme(phys_lead ~ 1,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(phys1)

#### model 2: add level 1 effects of state var ####
phys2 = lme(phys_lead ~ 1 + phys_state + avoid_state,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(phys2)

#### model 3: add level 2 effects ####
phys3 = lme(phys_lead ~ 1 + phys_state + avoid_state + cent_avoid,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(phys3)
intervals(phys3, which = "fixed")

phys3_noAR1 = lme(phys_lead ~ 1 + phys_state + avoid_state + cent_avoid,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(phys3)
intervals(phys3_noAR1)
anova(phys3, phys3_noAR1)

#### model 4: add random slopes for state variables ####
# phys4 = lme(phys_lead ~ 1 + phys_state + avoid_state + cent_avoid,
#             random =~ 1 + avoid_state|unique_id,
#             data=dat,
#             correlation= corCAR1(form=~beepcont),
#             na.action = na.exclude)
# summary(phys4)
# intervals(phys4) # NPD too complex go back to phys3 

qqnorm(residuals(phys3_noAR1))
qqnorm(phys3_noAR1, ~ranef(., level=1))
plot(phys3_noAR1)
```

## Phys Emotions
### emo to phys
```{r}
emophyscomb = lme(phys_lead ~ 1 + phys_state + anx_state +guilt_state,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(emophyscomb)

#### model 3: add level 2 effects ####
emophyscomb2 = lme(phys_lead ~ 1 + phys_state + anx_state + cent_anx + guilt_state+cent_guilt,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(emophyscomb2)
intervals(emophyscomb2)

#### model 4: add random slopes for state variables ####
emophyscomb3 = lme(phys_lead ~ 1 + phys_state + anx_state + cent_anx+ guilt_state+cent_guilt,
            random =~ 1 + anx_state+guilt_state|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(emophyscomb3)
intervals(emophyscomb3)

qqnorm(residuals(emophyscomb2))
qqnorm(emophyscomb2, ~ranef(., level=1))
plot(emophyscomb2)
```

### phys to anx
```{r}
summary(anx_phys2)
anx_phys2 = lme(anx_lead ~ 1 + anx_state + phys_state,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(anx_phys2)
intervals(anx_phys2)

### add level 2 ###
anx_phys2_noAR1 = lme(anx_lead ~ 1 + anx_state + phys_state+cent_phys,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(anx_phys2_noAR1)
intervals(anx_phys2_noAR1)

anova(anx_phys2, anx_phys2_noAR1)


qqnorm(residuals(anx_phys2_noAR1))
qqnorm(anx_phys2_noAR1, ~ranef(., level=1))
plot(anx_phys2_noAR1)
```

### phys to guilt
```{r}
guilt_phys = lme(guilt_lead ~ 1 + guilt_state + phys_state,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(guilt_phys)

### add level 2 ###
guilt_phys2 = lme(guilt_lead ~ 1 + guilt_state + phys_state+cent_phys,
            random =~ 1|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(guilt_phys2)
intervals(guilt_phys2)

guilt_phys2_noAR1 = lme(guilt_lead ~ 1 + guilt_state + phys_state+cent_phys,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(guilt_phys2_noAR1)
intervals(guilt_phys2_noAR1)
anova(guilt_phys2, guilt_phys2_noAR1)

### add random slopes ###
guilt_phys3 = lme(guilt_lead ~ 1 + guilt_state + phys_state+cent_phys,
            random =~ 1 + phys_state|unique_id,
            data=dat,
            correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
summary(guilt_phys3)
intervals(guilt_phys3) 

# anova(guilt_phys2, guilt_phys3) # go to more parsimonious model

## effect size
lme.dscore(guilt_phys3, dat, "nlme")


## pseudo R squared
r2mlm(guilt_phys2, bargraph = TRUE)

qqnorm(residuals(guilt_phys2_noAR1))
qqnorm(guilt_phys2_noAR1, ~ranef(., level=1))
plot(guilt_phys2_noAR1)
```



# RESULTS SUMMARY
## H1a summary

avoid3 = lme(avoid_lead ~ 1 + avoid_state + anx_state + anx_trait,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
            
```{r}
summary(avoid3)
intervals(avoid3)
exp(attr(avoid3$apVar, "Pars"))^2
lme.dscore(avoid3, dat, "nlme")
r2mlm(avoid3)
```

## H1b summary

anx3 = lme(anx_lead ~ 1 + anx_state + avoid_state + cent_avoid,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
            
```{r}
summary(anx3)
intervals(anx3)
exp(attr(anx3$apVar, "Pars"))^2
lme.dscore(anx3, dat, "nlme")
r2mlm(anx3)
```

## H2a summary

avoidcomb2 = lme(avoid_lead ~ 1 + avoid_state + anx_state + cent_anx + guilt_state+cent_guilt,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
            
```{r}
summary(avoidcomb2)
intervals(avoidcomb2)
lme.dscore(avoidcomb2, dat, "nlme")
r2mlm(avoidcomb2)

# get SE for random effects
exp(attr(avoidcomb2$apVar, "Pars"))^2

# double check
.1844799^2 # 0.03403283
.6609298^2 # .44 int
```

## H2b summary

guilt3 = lme(guilt_lead ~ 1 + guilt_state + avoid_state + cent_avoid,
            random =~ 1|unique_id,
            data=dat,
           # correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
            
```{r}
summary(guilt3)
intervals(guilt3)
lme.dscore(guilt3, dat, "nlme")
r2mlm(guilt3)

# get SE for random effects
exp(attr(guilt3$apVar, "Pars"))^2
# struct.unique_id1  0.31353857
# struct.unique_id1 0.04515177
# struct.unique_id1 0.75770315
# lsigma 0.83947259

# double check
0.5599452^2 # 0.3135386 intercept
0.2124895^2 # 0.04515179 avoid state slope
(-0.069)^2 # 0.004761 corr btwn random intercept and slop
0.9162274^2 # 0.8394726 resid
```

## H3a summary (no AR1)

avoid_phys2_noAR1 = lme(avoid_lead ~ 1 + avoid_state + phys_state+cent_phys,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
            
```{r}
summary(avoid_phys2_noAR1)
intervals(avoid_phys2_noAR1)
lme.dscore(avoid_phys2_noAR1, dat, "nlme")
# get SE for random effects
exp(attr(avoid_phys2_noAR1$apVar, "Pars"))^2
# struct.unique_id1  0.2423454
# lsigma 0.6588145

# double check
0.4922859^2 # 0.2423454 intercept
0.8116739^2 # 0.6588145 resid
r2mlm(avoid_phys2_noAR1)
```

## H3b summary (no AR1)

phys3_noAR1 = lme(phys_lead ~ 1 + phys_state + avoid_state + cent_avoid,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
            
```{r}
summary(phys3_noAR1)
intervals(phys3_noAR1)
lme.dscore(phys3_noAR1, dat, "nlme")
# get SE for random effects
exp(attr(phys3_noAR1$apVar, "Pars"))^2
# struct.unique_id1  0.2686168
# lsigma 0.6338602

# double check
0.5182825^2 # 0.2686167 intercept
0.7961534^2 # 0.6338602 resid
r2mlm(phys3_noAR1)
```

## H4
### emo to phys

emophyscomb3 = lme(phys_lead ~ 1 + phys_state + anx_state + cent_anx+ guilt_state+cent_guilt,
            random =~ 1 + anx_state+guilt_state|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
            
```{r}
summary(emophyscomb3)
intervals(emophyscomb3)
lme.dscore(emophyscomb3, dat, "nlme")
# get SE for random effects
exp(attr(emophyscomb3$apVar, "Pars"))^2
# struct.unique_id1  0.22326485
# 2 0.04886095
# 3 0.05187247
# lsigma 0.59445899

# double check
0.8825691^2 # 0.7789282 intercept
0.2268036^2 # 0.05143987 anx
0.2333909^2 # 0.05447131 guilt
0.7710117^2 # 0.594459 resid
r2mlm(emophyscomb3)
```

### phys to anx

anx_phys2_noAR1 = lme(anx_lead ~ 1 + anx_state + phys_state+cent_phys,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
            
```{r}
summary(anx_phys2_noAR1)
intervals(anx_phys2_noAR1)
lme.dscore(anx_phys2_noAR1, dat, "nlme")
# get SE for random effects
exp(attr(anx_phys2_noAR1$apVar, "Pars"))^2
# struct.unique_id1  0.2787632
# lsigma 0.8005689

# double check
0.5279803^2 # 0.2787632 intercept
0.8947451^2 # 0.8005688 resid
r2mlm(anx_phys2_noAR1)
```

### phys to guilt

guilt_phys2_noAR1 = lme(guilt_lead ~ 1 + guilt_state + phys_state+cent_phys,
            random =~ 1|unique_id,
            data=dat,
            #correlation= corCAR1(form=~beepcont),
            na.action = na.exclude)
            
```{r}
summary(guilt_phys2_noAR1)
intervals(guilt_phys2_noAR1)
lme.dscore(guilt_phys2_noAR1, dat, "nlme")
# get SE for random effects
exp(attr(guilt_phys2_noAR1$apVar, "Pars"))^2
# struct.unique_id1  0.2528268
# lsigma 0.8721444

# double check
0.5028189^2 # 0.2528268 intercept
0.9338867^2 # 0.8721444 resid
r2mlm(guilt_phys2_noAR1)
```

# corrs
```{r}
zcordf <- dat %>% 
  filter(beep<5) %>% 
  group_by(unique_id) %>% 
  summarise(anx_mean=mean(anx), guilt_mean=mean(guilt), avoid_mean=mean(avoidemo), phys_mean=mean(physuncomfort)) %>% 
  ungroup()

corr.test(zcordf[-1], adjust = "holm")
```

# demos
## gender
4 men, 124 women, 1 not listed, 1 NA
```{r}
dim(survey_current)
table(survey_current$gender, useNA = "ifany")
# 4 men, 102 women, 1 not listed, 1 NA
prop.table(table(survey_current$gender, useNA = "ifany"))*100
```

## race
```{r}
# table(dh3survey$ethnicity, useNA = "ifany")
survey_current$ethnicity <- dplyr::recode(survey_current$ethnicity, "0"="American Indian or Alaskan Native", "1" = "AAPI", "2" = "Black, not of Hispanic origin (includes African American)", "3"="Hispanic", "4"="Multiracial, Biracial, Multiple Broad Categories", "5"="White, not of Hispanic origin (includes Caucasian, European American)", "6"="Not listed")
table(survey_current$ethnicity, useNA = "ifany")
# AAPI = 6
# Native American = 1
# Black = 3
# Hispanic = 6
# Multi-/bi-racial = 4
# White = 88
prop.table(table(survey_current$ethnicity, useNA = "ifany"))*100

racedf<- data.frame(
  race = unlist(as.data.frame(table(survey_current$ethnicity))[1]),
  n = unlist(as.data.frame(table(survey_current$ethnicity))[2]),
  prop = sapply(unlist(as.data.frame(table(survey_current$ethnicity))[2]), function(x) round((x/dim(survey_current)[1])*100, 2)))

racedf[,1] <- c("AAPI", "Native American", "Black", "Hispanic", "Multiracial", "White")

racedf2 <- racedf %>% 
  mutate(csum=rev(cumsum(rev(n))),
         pos = n/2 + lead(csum, 1),
         pos = if_else(is.na(pos), n/2, pos))
racedf %>% mutate(csum=rev(cumsum(rev(n))))

library(ggrepel)
racepie <- racedf %>% ggplot(aes(x = "" , y = n, fill = fct_inorder(race))) +
  geom_col(width = 1, color = 1) +
  coord_polar(theta = "y") +
  scale_fill_manual(values = c("#FF9AA2", "#ffffcc", "#FFDAC1", "#E2F0CB","#B5EAD7", "#C7CEEA")) +
 # scale_fill_brewer(palette = "Pastel1") +
  geom_label_repel(data = racedf2,
                   aes(y = pos, label = paste0(prop, "%")),
                   size = 6, nudge_x = .6, show.legend = FALSE) +
  guides(fill = guide_legend(title = "")) +
  theme_void()+
  theme(legend.text=element_text(size=16))

# dir.create("./APSfigs") # create a path
# ggsave(racepie, filename = "APSfigs/racepie.png", bg = "transparent", width = 8, height =4)

table(survey_current$total_school_years)
describe(survey_current$total_school_years) # 16.21 (min = 11, max = 22)  
survey_current %>% filter(total_school_years<12)
# DH024 17
# DH064 15
survey_current[which(survey_current$unique_id=="DH024"), which(colnames(survey_current)=="total_school_years")] <- 17
survey_current[which(survey_current$unique_id=="DH064"), which(colnames(survey_current)=="total_school_years")] <- 15

table(survey_current$work_status, useNA = "ifany")
# 29 not working, 52 working full time, 26 working part time, 1 NA
round(prop.table(table(survey_current$work_status, useNA = "ifany"))*100,2)
```

## diagnosis
```{r}
table(dh3diagnoses$CurrentDx_general, useNA = "ifany")
# AN = 35, ARFID = 0, BED = 4, BN = 17, OSFED = 52

prop.table(table(dh3diagnoses$CurrentDx_general, useNA = "ifany"))*100

table(dh3diagnoses$CurrentDX_specific, useNA = "ifany")
table(dh3diagnoses[which(dh3diagnoses$CurrentDx_general=="NoDx"),]$PastDX_general, useNA = "ifany")
```

## age
```{r}
describe(dh3survey$age) # min 18, max 62, mean = 29.32 (SD = 9.20)
```

## tx hist
```{r}
table(dh3survey$treatmentq_1, useNA = "ifany")
# 0 = 42, 1 = 66
round(prop.table(table(dh3survey$treatmentq_1, useNA = "ifany"))*100,1)
```

## time eaten
```{r}
test = dat[which(dat$beep<5),]
str(test$timesinceat) # integer
(colMeans(is.na(test)))*100 # 44.04
describe(test$timesinceat, na.rm = TRUE) # 199.07 (3.31), sd = 253.46 (4 hours) min = 0, max 1000 (over 16 hours)
```

# power
```{r}
ema.powercurve(NumbPart = 108,
               NumbResp = 100,
               days=25,
               respday = 4,
               Est_ICC = .5)
```

# add table with m, sds, iccs for the 4 vars at group level
```{r}
describe(na.omit(dat$anx)) # m = 2.87 SD = 1.39
describe(na.omit(dat$guilt)) # m = 2.99, SD = 1.45
describe(na.omit(dat$avoidemo)) # m = 2.99, SD = 1.60
describe(na.omit(dat$physuncomfort)) # m = 2.82, SD = 1.58
```

# variability plots
## long
```{r}
dat_plot <- dat
dat_plot <- dat_plot[-which(dat_plot$beep==5),]

dat_plot <- dat_plot[,c(1,3:5,8:10,13)]
dat_plot <- dat_plot %>% dplyr::rename(avoid=avoidemo, phys=physuncomfort)
dat_plot <- dat_plot %>% pivot_longer(
  cols=!1:4,
  names_to = "var",
  values_to = "value"
)
```

## idio plot
```{r}
idiovar_plot <- dat_plot %>% 
  filter(!is.na(unique_id)) %>% 
  filter(beepcont<101) %>% 
  ggplot(aes(x=beepcont, y=value, color=var)) +
  geom_line() +
  theme_classic() +
  labs(x="time", color="variable") +
  theme(axis.text.x = element_blank(),
        legend.position = "bottom") +
  scale_color_manual(values=c("#E69F00", "#56B4E9", "#009E73","#CC79A7")) +
  #scale_color_discrete(name = "variable") +
  facet_wrap(~unique_id) 
# ggsave(dxwaffle, filename = "APSfigs/dxwaffle.png", bg = "transparent", width = 4, height =4)
ggsave(idiovar_plot, filename = "idiovarplot_3oct2023.png", bg="transparent", width=13)
```

## spaghetti plot
```{r}
dat_plot$var <- factor(dat_plot$var, levels=c("avoid", "phys", "anx", "guilt"))
spaghetti <- dat_plot %>% 
  filter(beepcont<101) %>% 
  ggplot(aes(x=beepcont, y=value, group=unique_id, color=var)) + 
  labs(x="time", y="", color="variable")+
  stat_smooth(method = "lm", se = FALSE, size=.5)+
  theme_classic() +
  theme(axis.text.x = element_blank(),
        legend.position = "bottom")+
  scale_color_manual(values=c("#56B4E9", "#CC79A7", "#E69F00","#009E73"))+
  facet_wrap(vars(var), ncol=2)

ggsave(spaghetti, filename = "spaghettiplot_current.png", bg = "transparent", width = 9)
```


## group plot
```{r}
dat_plot2 <- dat_plot[4:6]

dat_plot2 <- dat_plot2 %>% 
  dplyr::group_by(var, beepcont) %>% 
  dplyr::summarise(mean=mean(value))

group_plot <- dat_plot2 %>% 
  dplyr::filter(beepcont<101) %>% 
  ggplot(aes(x=beepcont, y=mean, color=var)) +
  geom_line() +
  theme_classic() +
  labs(x="time", color="variable") +
  theme(axis.text.x = element_blank(),
        legend.position = "bottom") +
  scale_color_manual(values=c("#E69F00", "#56B4E9", "#009E73","#CC79A7")) #+
  # coord_cartesian(ylim = c(0, 6))

ggsave(group_plot, filename = "groupplot_3oct2023.png", bg = "transparent", width = 9, height =5)
```

# save
```{r}
# save.image("premealemotions.RData")

#### deidentify survey ####
dh3survey_deident <- dh3survey[c(1,3:7,10:12, 15:17, 41)]

write.csv(dh3survey_deident, "demos_deidentified.csv", row.names = FALSE)
```

# reliability

R2 Comment 11   
*I appreciated the authors' willingness to work on providing reliability information for their multilevel data using variance components analysis. I believe that the authors need to use the different sources of variance in the ANOVA table they reported in their letter to the editor, computing reliability based on those sources of variance. The mlr (or multilevel.reliability) function should be able to make those calculations automatically (see Revelle & Wilt's "Analyzing dynamic data: A tutorial" paper): Analyzing dynamic data: A tutorial - ScienceDirect). They can also see: multilevel.reliability function - RDocumentation*

```{r}
load("premealemotions.RData")
library(tidyverse)
library(psych)
library(VCA)

# table(dat$unique_id, useNA = "ifany")
# keep participants with 125
# tt <- table(dat$unique_id)
# df2 <- dat[dat$unique_id %in% names(tt[tt == 125]), ]
# table(df2$unique_id)
# df2<-droplevels(df2)
test <- dat
nth.delete<-function(dataframe, n)dataframe[-(seq(n,to=nrow(dataframe),by=n)),]
test <- nth.delete(test, 5)
```

## mlr
```{r}
# rel
emorel <- mlr(test, "unique_id", "beepcont", items = c("anx", "guilt", "physuncomfort", "avoidemo"), aov=FALSE, lmer = TRUE)
# Warning: diag(.) had 0 or NA entries; non-finite result is doubtfulAt least one item had no variance  when finding alpha for subject = DH029. Proceed with caution
# View(test[which(test$unique_id=="DH029"), c(1,5,8:10,13)]) # physuncomfort all 6s

alphadf <- emorel$alpha[,c(1,3)]
 
select <- c("anx", "guilt", "avoidemo", "physuncomfort")
emotionkeys <- list(anxiety="anx",
                    guilt="guilt", 
                    avoid = "avoidemo",
                    phys = "physuncomfort")
emotionscores <- scoreItems(keys = emotionkeys, items = test[select], min = 0 , max = 6) # Number of categories should be increased  in order to count frequencies. 

# emotionscores$scores
emodf <- cbind(test[c(1,5)],emotionscores$scores)
mlPlot(emodf, grp="unique_id", Time = "beepcont", items = 3:6, type = "p")

stats.emo <- statsBy(emodf, group = "unique_id", cors = TRUE)
stats.emo$within
stats.emo$pooled
stats.emo$ICC2
# anxiety = 0.9893515
# guilt = 0.9890796 
# phys = 0.9923235 
# avoid emo = 0.9924608 

# within person pwg reflect pooled number of cases within groups
stats.emo$pwg # < .001

# pooled within group correlations rwg confidence intervals of pooled within group correlations ci.wg
stats.emo$ci.wg

# etawg: correlation of the data with the within group values
stats.emo$etawg
# anx=  0.7522609 
# guilt = 0.7563587 
# phys = 0.6960401 
# avoid = 0.6927954

# between person pbg (number of groups)
stats.emo$pbg
# sample sized weighted between group correlations rbg
# confidence intervals of rbg c.bg
stats.emo$ci.bg

# etabg: correlation of the data with group means 
stats.emo$etabg
#  anx = 0.6588653   
# guilt = 0.6541571   
# avoid = 0.7180029   
# phys =0.7211342 

ar <- autoR(emodf[2:6], group = emodf["unique_id"])
# View(ar$autoR)
alphadf <- cbind(alphadf,
                 stats.emo$within,
                 ar$autoR[,2:5])


# reliability of average of all ratings across all times and times (fixed time effects) RKF
emorel$RkF # 0.9990704 # the reliability of individual differences over k fixed time points and m multiple items

# generalizability of a single time point across all items (random item effects) R1R
emorel$R1R # 0.6585325

# generalizability of average time points across all items (random effects) RKR
emorel$RkR # 0.9958689

# Generalizability of change scores RC
emorel$Rc # 0.7737115

# generalizability of K random effects nested RKRN
emorel$RkRn # 0.99586

# reliability of between person differences averaged over items RCN 
emorel$Rcn # 0.5898699 # generalizability of within person variations averaged over items (time nested within people)

# alpha = Within subject alpha over items and time.
emorel$alpha
```


```{r}
anx_test <- test %>% filter(!is.na(anx))

varPlot(form=anx~unique_id, Data=anx_test, 
        type=1,
        VARtype = "CV", 
        htab = 0.2, 
        YLabel = list(text="anxiety value",line=2, cex=1.25),
        Points = list(cex=.2),
        ylim = c(1,6),
        VarLab=list(list(cex=.5,srt=90), list(srt=90)))

# dat_subset$unique_id <- factor(dat_subset$unique_id)
vcafit_anx <- fitVCA(anx~unique_id, anx_test)
vcafit_anx
inf.anx <- VCAinference(vcafit_anx, VarVC=TRUE)
inf.anx

dat_subset_DH001 <- dat_subset %>% filter(unique_id=="DH001")
varPlot(form=anx~unique_id/day, Data=dat_subset_DH001, MeanLine = list(var="int"))

itemNames <- c("anx", "guilt","avoidemo", "physuncomfort")
test %>%
    group_by(unique_id) %>%    
     summarise(alpha = alpha(across(all_of(itemNames)))$total$raw_alpha, 
     omega = ci.reliability(across(all_of(itemNames)), 
    type = "omega", interval.type = "none")$est, .groups = 'drop')

alpha(dat_subset[,which(colnames(dat_subset)=="anx")]) # .86
psychometric::alpha(dat_subset$anx)

# DH029 no variance

# RKF reliability of average of all ratings across all items and times
# sigma sq p population variance
# sigma sq i variance between items
# sigma sq t variance over time

vcafit_anxtime <- fitVCA(anx~unique_id:beepcont, anx_test)

```

## try 2
```{r}
#### anx ####
anx.rel <- mlr(anx_test, grp="unique_id", Time = "beepcont", items = "anx", aov = FALSE, lmer = TRUE, lme=FALSE, long = TRUE, na.action = "na.omit")

# try multiple items
mg <- multilevel.reliability(test,grp="unique_id",Time="beepcont",items= c("anx","guilt","avoidemo", "physuncomfort"), plot=TRUE)
```

## test
```{r}
set.seed(42)
x <- sim.multi(n.obs = 4, nvar = 4, nfact = 2, days = 6, ntrials = 6, plot=TRUE, phi.i = c(-.7, 0, 0, .7), loading = .6)
# View(x)
raw <- round(x[3:8])
# View(raw)
raw[1:4] <- raw[1:4] + 6 
# make a 'Fat' version
XFat <- reshape(raw, idvar="id", timevar = "time", times = 1:4, direction = "wide")
# View(XFat)
# now make it wide
XWide <- reshape(XFat, idvar = "id", varying = 2:25, direction = "long")
# View(XWide)
Xwide <- arrange(XWide, id)

# add trait information
traits <- data.frame(id=1:4, extraversion=c(5,10,15,20), neuroticism=c(10,5,15,10))
Xwide.traits <- merge(Xwide, traits, by="id")
```

## test 2
```{r}
shrout <- structure(list(Person = c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 
5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L), Time = c(1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 
4L, 4L), Item1 = c(2L, 3L, 6L, 3L, 7L, 3L, 5L, 6L, 3L, 8L, 4L, 
4L, 7L, 5L, 6L, 1L, 5L, 8L, 8L, 6L), Item2 = c(3L, 4L, 6L, 4L, 
8L, 3L, 7L, 7L, 5L, 8L, 2L, 6L, 8L, 6L, 7L, 3L, 9L, 9L, 7L, 8L
), Item3 = c(6L, 4L, 5L, 3L, 7L, 4L, 7L, 8L, 9L, 9L, 5L, 7L, 
9L, 7L, 8L, 4L, 7L, 9L, 9L, 6L)), .Names = c("Person", "Time", 
"Item1", "Item2", "Item3"), class = "data.frame", row.names = c(NA, 
-20L))

mg <- mlr(shrout,grp="Person",Time="Time",items=3:5) 
mg <- multilevel.reliability(shrout,grp="Person",Time="Time",items= c("Item1","Item2","Item3"),plot=TRUE)
```




# alpha
## function
```{r}
within_between_alpha <- function(data,participant){
  # Load libraries
  require(psych)
  #require(alpha)
  require(dplyr)
  
  # Get between-person alpha
  between_person_alpha = psych::alpha(as.data.frame(lapply(dplyr::select(data,-participant),as.numeric)),check.keys=TRUE)$total
  
  # Get within-person alpha
  within_person_alphas = apply(unique(data[participant]),1,function(x){
    dplyr::select(subset(data,data[participant]==x),-participant)
    psych::alpha(as.data.frame(lapply(select(subset(data,data[participant]==x),-participant),as.numeric)),check.keys=TRUE)$total
  })

  # Create results
  results = data.frame(matrix(nrow=3,ncol=length(between_person_alpha)+1))
  colnames(results) = c('type',colnames(between_person_alpha))
  
  # Store results
  results$type = c('between_person','within_person_mean','within_person_median')
  results[1,2:(length(between_person_alpha)+1)] = between_person_alpha
  results[2,2:(length(between_person_alpha)+1)] = apply(bind_rows(within_person_alphas),2,function(x) mean(x,na.rm=T))
  results[3,2:(length(between_person_alpha)+1)] = apply(bind_rows(within_person_alphas),2,function(x) median(x,na.rm=T))
  
  # Return results
  return(results)
}

#### EXAMPLE ####

# Negative affect
# negative_affect_data <- my_data_ml[,c('Participant','affect_angstig','affect_eenzaam','affect_gestrest','affect_schuldig','affect_somber','affect_onzeker')]
# alpha_na <- within_between_alpha(negative_affect_data,'Participant')
```

## apply
```{r}
load("premealemotions.RData")
library(alpha)
anxalphadat <- dat %>% filter(beep<5) %>% dplyr::select(unique_id, anx)
within_between_alpha(anxalphadat, 'unique_id')

# between_person_alpha =
psych::alpha(as.data.frame(lapply(dplyr::select(anxalphadat,-'unique_id'),as.numeric)),check.keys=TRUE)$total
psych::alpha(anxalphadat)
```


